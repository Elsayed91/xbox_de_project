kind: Deployment
apiVersion: apps/v1
metadata:
  name: airflow
spec:
  selector:
    matchLabels:
      app: airflow
  template:
    metadata:
      labels:
        app: airflow
    spec:
      securityContext:
        runAsUser: 50000
        fsGroup: 0
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - airflow
            topologyKey: kubernetes.io/hostname
      initContainers:
        - name: run-airflow-migrations
          image: eu.gcr.io/${PROJECT}/airflow
          imagePullPolicy: Always
          command: ["/bin/bash", "/opt/airflow/init.sh"]
          env:
            - name: PYTHONUNBUFFERED
              value: "1"
          envFrom:
            - secretRef:
                name: airflow-secret
          resources: {}
          volumeMounts:
            - name: config
              mountPath: "/opt/airflow/airflow.cfg"
              subPath: airflow.cfg
      containers:
        - name: scheduler
          image: eu.gcr.io/${PROJECT}/airflow:latest
          imagePullPolicy: IfNotPresent
          args:
            - bash
            - -c
            - exec airflow scheduler
          envFrom:
            - secretRef:
                name: airflow-secret
          ports:
            - name: worker-logs
              containerPort: 8793
          livenessProbe:
            initialDelaySeconds: 10
            timeoutSeconds: 20
            failureThreshold: 5
            periodSeconds: 60
            exec:
              command:
                - sh
                - -c
                - |
                  CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR exec /entrypoint \
                  airflow jobs check --job-type SchedulerJob --hostname $(hostname)
          resources: {}
          volumeMounts:
            - name: config
              mountPath: /opt/airflow/pod_templates/pod_template_file.yaml
              subPath: pod_template_file.yaml
            - name: logs
              mountPath: "/opt/airflow/logs"
            - name: dags
              mountPath: /git
            - name: config
              mountPath: "/opt/airflow/airflow.cfg"
              subPath: airflow.cfg
            - name: config
              mountPath: "/opt/airflow/config/airflow_local_settings.py"
              subPath: airflow_local_settings.py
        - name: git-sync
          image: k8s.gcr.io/git-sync/git-sync:v3.6.2
          imagePullPolicy: Always
          securityContext:
            runAsUser: 65533
          envFrom:
            - configMapRef:
                name: git-env
          resources: {}
          volumeMounts:
            - name: dags
              mountPath: /git
        - name: webserver
          image: eu.gcr.io/${PROJECT}/airflow #apache/airflow:2.5.0
          imagePullPolicy: Always
          args:
            - bash
            - -c
            - exec airflow webserver
          envFrom:
            - secretRef:
                name: airflow-secret
          livenessProbe:
            failureThreshold: 20
            httpGet:
              path: /health
              port: 8080
              scheme: HTTP
          ports:
            - name: airflow-ui
              containerPort: 8080
          resources: {}
      nodeSelector:
        iam.gke.io/gke-metadata-server-enabled: "true"
      volumes:
        - name: config
          configMap:
            name: airflow-config
        - name: logs
          emptyDir: {}
        - name: dags
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: airflow-headless-service
  labels:
    app: airflow
spec:
  type: ClusterIP
  clusterIP: None
  selector:
    app: airflow
  ports:
    - name: task-logs
      port: 8793
      targetPort: 8793
---
kind: Service
apiVersion: v1
metadata:
  name: airflow-webserver
  labels:
    app: airflow
spec:
  type: NodePort
  selector:
    app: airflow
  ports:
    - name: airflow-ui
      port: 8080
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: git-env
data:
  GIT_SYNC_REV: "HEAD"
  GIT_SYNC_BRANCH: "main"
  GIT_SYNC_REPO: ${GIT_SYNC_REPO}
  GIT_SYNC_DEPTH: "1"
  GIT_SYNC_ROOT: "/git"
  GIT_SYNC_DEST: "repo"
  # GIT_SYNC_USERNAME: "Liftingthedata"
  GIT_SYNC_WAIT: "5"
  GIT_SYNC_MAX_SYNC_FAILURES: "0"
  GIT_KNOWN_HOSTS: "false"
  # GIT_SYNC_PASSWORD: ${GIT_SYNC_PASSWORD}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
data:
  TWITTER_DATASET: ${TWITTER_DATASET}
  VGCHARTZ_DATASET: ${VGCHARTZ_DATASET}
  METACRITIC_DATASET: ${METACRITIC_DATASET}
  DATA_BUCKET: ${DATA_BUCKET}
  DATA_VOLUME_PATH: ${DATA_VOLUME_PATH}
  GOOGLE_CLOUD_PROJECT: ${PROJECT}
  
  airflow.cfg: |-
    [celery]
    flower_url_prefix = /
    worker_concurrency = 1

    [celery_kubernetes_executor]
    kubernetes_queue = kubernetes

    [core]
    colored_console_log = False
    dags_folder = /git/repo/airflow/dags/
    executor = LocalExecutor
    load_examples = False
    remote_logging = False
    # plugins_folder = {AIRFLOW_HOME}/utilities

    [elasticsearch]
    json_format = True
    log_id_template = {dag_id}_{task_id}_{execution_date}_{try_number}

    [elasticsearch_configs]
    max_retries = 3
    retry_timeout = True
    timeout = 30

    [kerberos]
    ccache = /var/kerberos-ccache/cache
    keytab = /etc/airflow.keytab
    principal = airflow@FOO.COM
    reinit_frequency = 3600

    [kubernetes_executor]
    airflow_configmap = airflow-airflow-config
    airflow_local_settings_configmap = airflow-airflow-config
    multi_namespace_mode = False
    namespace = airflow
    pod_template_file = /opt/airflow/pod_templates/pod_template_file.yaml
    worker_container_repository = apache/airflow
    worker_container_tag = 2.5.0

    [logging]
    colored_console_log = False
    remote_logging = False

    [metrics]
    statsd_host = metric-exporters.default.svc.cluster.local
    statsd_on = True
    statsd_port = 9125
    statsd_prefix = airflow

    [scheduler]
    run_duration = 41460
    standalone_dag_processor = False
    statsd_host = metric-exporters.default.svc.cluster.local
    statsd_on = True
    statsd_port = 9125
    statsd_prefix = airflow

    [webserver]
    enable_proxy_fix = True
    rbac = True

    [kubernetes_job_operator]
    # The task kube resources delete policy. Can be: Never, Always, IfFailed, IfSucceeded
    delete_policy=Never
    # The default object type to execute with (legacy, or image). Can be: Pod, Job
    default_execution_object=Pod

    # Logs
    detect_kubernetes_log_level=True
    show_kubernetes_timestamps=False
    # Shows the runner id in the log (for all runner logs.)
    show_runner_id=False

    # Tasks (Defaults)
    # Wait to first connect to kubernetes.
    startup_timeout_seconds=120
    # if true, will parse the body when building the dag. Otherwise only while executing.
    validate_body_on_init=False

    # Comma seperated list of where to look for the kube config file. Will be added to the top
    # of the search list, in order.
    kube_config_extra_locations=

  airflow_local_settings.py: |2


    from airflow.www.utils import UIAlert

    DASHBOARD_UIALERTS = [
      UIAlert(
        'Usage of a dynamic webserver secret key detected. We recommend a static webserver secret key instead.'
        ' See the <a href='
        '"https://airflow.apache.org/docs/helm-chart/stable/production-guide.html#webserver-secret-key">'
        'Helm Chart Production Guide</a> for more details.',
        category="warning",
        roles=["Admin"],
        html=True,
      ) ]
---
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: airflow-secret
stringData:
  AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW__CORE__FERNET_KEY}
  AIRFLOW__CORE__SQL_ALCHEMY_CONN: "postgresql://${POSTGRES_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST}:5432/${AIRFLOW_DB}"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql://${POSTGRES_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST}:5432/${AIRFLOW_DB}"
  AIRFLOW_CONN_AIRFLOW_DB: "postgresql://${POSTGRES_USER}:${AIRFLOW_DB_PASSWORD}@${POSTGRES_HOST}:5432/${AIRFLOW_DB}"
  AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW__WEBSERVER__SECRET_KEY}
  AIRFLOW_ADMIN_USER: ${AIRFLOW_ADMIN_USER}
  AIRFLOW_ADMIN_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD}
  AIRFLOW_VIEWER_USER: ${AIRFLOW_VIEWER_USER}
  AIRFLOW_VIEWER_PASSWORD: ${AIRFLOW_VIEWER_PASSWORD}